{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "% An example to get the SEED dataset\n",
        "% Just an example, you should change as you need.\n",
        "#preprocessing\n",
        "\n",
        "% band pass to 4-47 Hz, standardization\n",
        "% 15 subjects - 3 sessions - 15 trials\n",
        "% sample rate 200 Hz\n",
        "% label: -1 for negative, 0 for neutral and +1 for positive\n",
        "% label changes to 0 1 2\n",
        "\n",
        "% Save file S1_1_1  (channels, samples)\n",
        "\n",
        "file_list = load('./seed_file/namefile_list.mat');\n",
        "file_list = file_list.namefile_list;\n",
        "short_name = load('./seed_file/short_name.mat');\n",
        "short_name = short_name.short_name;\n",
        "label = load('./seed_file/label.mat');\n",
        "fc = 200;\n",
        "Wl = 4; Wh = 47;\n",
        "Wn = [Wl*2 Wh*2]/fc;\n",
        "[b,a]=cheby2(6,60,Wn);\n",
        "\n",
        "for i = 1:15 % subject\n",
        "    for j = 1:3 % session\n",
        "        file_pre = file_list(i,j);\n",
        "        file_path = strcat('/Datasets/SEED/Preprocessed_EEG/',file_pre,'.mat');\n",
        "        session_data = load(file_path);\n",
        "        for k = 1:15 % trial\n",
        "            trial_data = eval(strcat('session_data.',short_name(i),'_eeg',num2str(k)));\n",
        "            % trial_data = filtfilt(b,a,trial_data);\n",
        "            trial_mean = mean(trial_data,2);\n",
        "            trial_std = std(trial_data,1,2);\n",
        "            trial_data = (trial_data-trial_mean)./trial_std;\n",
        "            trial_data = filtfilt(b,a,trial_data);\n",
        "            trial_label = label.label(k);\n",
        "            saveDir = strcat('/Datasets/SEED/seed_save/S',num2str(i),'_',num2str(j),'_',num2str(k),'.mat');\n",
        "            save(saveDir,'trial_data','trial_label');\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ELjignffV8Gd",
        "outputId": "0dcec9ef-b6e9-4bfc-eaff-943dd1541c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-1-bbfc138122c1>, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-bbfc138122c1>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    Wn = [Wl*2 Wh*2]/fc;\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Ew0m1hC5Fslf",
        "outputId": "afb2ee0c-1307-4b9e-a127-013bcdee5fff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'einops'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c60a374505b0>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"\n",
        "EEG conformer\n",
        "\n",
        "Test SEED data 1 second\n",
        "perform strict 5-fold cross validation\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "gpus = [1]\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import random\n",
        "import itertools\n",
        "import datetime\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "import scipy.io\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "import torch.autograd as autograd\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.backends import cudnn\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, emb_size=40):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eegnet = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, (1, 125), (1, 1)),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8, 16, (22, 1), (1, 1)),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 4), (1, 4)),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(16, 16, (1, 16), (1, 1)),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 8), (1, 8)),\n",
        "            nn.Dropout2d(0.5)\n",
        "        )\n",
        "\n",
        "        self.shallownet = nn.Sequential(\n",
        "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
        "            nn.Conv2d(40, 40, (62, 1), (1, 1)),\n",
        "            nn.BatchNorm2d(40),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d((1, 75), (1, 15)),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # 5 is better than 1\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "\n",
        "        x = self.shallownet(x)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 num_heads=5,\n",
        "                 drop_p=0.5,\n",
        "                 forward_expansion=4,\n",
        "                 forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, n_classes):\n",
        "        super().__init__()\n",
        "        self.cov = nn.Sequential(\n",
        "            nn.Conv1d(190, 1, 1, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.clshead = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes)\n",
        "        )\n",
        "        self.clshead_fc = nn.Sequential(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(280, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return x, out\n",
        "\n",
        "\n",
        "# ! Rethink the use of Transformer for EEG signal\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self, emb_size=40, depth=6, n_classes=4, **kwargs):\n",
        "        super().__init__(\n",
        "\n",
        "            PatchEmbedding(emb_size),\n",
        "            TransformerEncoder(depth, emb_size),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "class ExGAN():\n",
        "    def __init__(self, nsub, fold):\n",
        "        super(ExGAN, self).__init__()\n",
        "        self.batch_size = 200\n",
        "        self.n_epochs = 600  #1000\n",
        "        self.img_height = 22\n",
        "        self.img_width = 600\n",
        "        self.channels = 1\n",
        "        self.c_dim = 4\n",
        "        self.lr = 0.0002\n",
        "        self.b1 = 0.5\n",
        "        self.b2 = 0.999\n",
        "        self.alpha = 0.0002\n",
        "        self.dimension = (190, 50)\n",
        "        self.nSub = nsub\n",
        "\n",
        "        self.start_epoch = 0\n",
        "        self.root = '/Data/SEED/seed_syh/data_cv5fold/'\n",
        "\n",
        "        self.pretrain = False\n",
        "\n",
        "        self.log_write = open(\"/Code/CT/results/D_base_comp/seed/5-fold/real/log_subject%d_fold%d.txt\" % (self.nSub, fold+1), \"w\")\n",
        "\n",
        "        self.img_shape = (self.channels, self.img_height, self.img_width)\n",
        "\n",
        "        self.Tensor = torch.cuda.FloatTensor\n",
        "        self.LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
        "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
        "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        self.model = ViT().cuda()\n",
        "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
        "        self.model = self.model.cuda()\n",
        "\n",
        "        self.centers = {}\n",
        "\n",
        "    def interaug(self, timg, label):\n",
        "        aug_data = []\n",
        "        aug_label = []\n",
        "        for cls4aug in range(4):\n",
        "            cls_idx = np.where(label == cls4aug + 1)\n",
        "            tmp_data = timg[cls_idx]\n",
        "            tmp_label = label[cls_idx]\n",
        "\n",
        "            tmp_aug_data = np.zeros((int(self.batch_size / 4), 1, 22, 1000))\n",
        "            for ri in range(int(self.batch_size / 4)):\n",
        "                for rj in range(8):\n",
        "                    rand_idx = np.random.randint(0, tmp_data.shape[0], 8)\n",
        "                    tmp_aug_data[ri, :, :, rj * 125:(rj + 1) * 125] = tmp_data[rand_idx[rj], :, :,\n",
        "                                                                      rj * 125:(rj + 1) * 125]\n",
        "\n",
        "            aug_data.append(tmp_aug_data)\n",
        "            aug_label.append(tmp_label[:int(self.batch_size / 4)])\n",
        "        aug_data = np.concatenate(aug_data)\n",
        "        aug_label = np.concatenate(aug_label)\n",
        "        aug_shuffle = np.random.permutation(len(aug_data))\n",
        "        aug_data = aug_data[aug_shuffle, :, :]\n",
        "        aug_label = aug_label[aug_shuffle]\n",
        "\n",
        "        aug_data = torch.from_numpy(aug_data).cuda()\n",
        "        aug_data = aug_data.float()\n",
        "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
        "        aug_label = aug_label.long()\n",
        "        return aug_data, aug_label\n",
        "\n",
        "    def get_source_data(self, fold):\n",
        "\n",
        "        self.all_data = np.load(self.root + 'S%d_session1.npy' % self.nSub, allow_pickle=True)\n",
        "        self.all_label = np.load(self.root + 'S%d_session1_label.npy' % self.nSub, allow_pickle=True)\n",
        "        self.train_data = []\n",
        "        self.train_label = []\n",
        "        self.test_data = []\n",
        "        self.test_label = []\n",
        "\n",
        "        for tri in range(np.shape(self.all_data)[0]):\n",
        "            tmp_tri = np.array(self.all_data[tri])\n",
        "            tmp_tri_label = np.array(self.all_label[tri])\n",
        "\n",
        "            one_fold_num = np.shape(tmp_tri)[0] // 5\n",
        "            tri_num =  one_fold_num * 5\n",
        "            tmp_tri_idx = np.arange(tri_num)\n",
        "            test_idx = np.arange(one_fold_num * fold, one_fold_num * (fold+1))\n",
        "            train_idx = np.delete(tmp_tri_idx, test_idx)\n",
        "\n",
        "            self.train_data.append(tmp_tri[train_idx])\n",
        "            self.train_label.append(tmp_tri_label[train_idx])\n",
        "            self.test_data.append(tmp_tri[test_idx])\n",
        "            self.test_label.append(tmp_tri_label[test_idx])\n",
        "\n",
        "        self.train_data = np.concatenate(self.train_data)\n",
        "        self.train_data = np.expand_dims(self.train_data, axis=1)\n",
        "        self.train_label = np.concatenate(self.train_label)\n",
        "        self.test_data = np.concatenate(self.test_data)\n",
        "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
        "        self.test_label = np.concatenate(self.test_label)\n",
        "\n",
        "        shuffle_num = np.random.permutation(len(self.train_data))\n",
        "        self.train_data = self.train_data[shuffle_num, :, :, :]\n",
        "        self.train_label = self.train_label[shuffle_num]\n",
        "\n",
        "        # standardize\n",
        "        target_mean = np.mean(self.train_data)\n",
        "        target_std = np.std(self.train_data)\n",
        "        self.train_data = (self.train_data - target_mean) / target_std\n",
        "        self.test_data = (self.test_data - target_mean) / target_std\n",
        "\n",
        "        return self.train_data, self.train_label, self.test_data, self.test_label\n",
        "\n",
        "    def update_lr(self, optimizer, lr):\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def aug(self, img, label):\n",
        "        aug_data = []\n",
        "        aug_label = []\n",
        "        for cls4aug in range(4):\n",
        "            cls_idx = np.where(label == cls4aug + 1)\n",
        "            tmp_data = img[cls_idx]\n",
        "            tmp_label = label[cls_idx]\n",
        "\n",
        "            tmp_aug_data = np.zeros(tmp_data.shape)\n",
        "            for ri in range(tmp_data.shape[0]):\n",
        "                for rj in range(8):\n",
        "                    rand_idx = np.random.randint(0, tmp_data.shape[0], 8)\n",
        "                    tmp_aug_data[ri, :, :, rj * 125:(rj + 1) * 125] = tmp_data[rand_idx[rj], :, :, rj * 125:(rj + 1) * 125]\n",
        "\n",
        "            aug_data.append(tmp_aug_data)\n",
        "            aug_label.append(tmp_label)\n",
        "        aug_data = np.concatenate(aug_data)\n",
        "        aug_label = np.concatenate(aug_label)\n",
        "        aug_shuffle = np.random.permutation(len(aug_data))\n",
        "        aug_data = aug_data[aug_shuffle, :, :]\n",
        "        aug_label = aug_label[aug_shuffle]\n",
        "\n",
        "        return aug_data, aug_label\n",
        "\n",
        "    def update_centers(self, feature, label):\n",
        "            deltac = {}\n",
        "            count = {}\n",
        "            count[0] = 0\n",
        "            for i in range(len(label)):\n",
        "                l = label[i]\n",
        "                if l in deltac:\n",
        "                    deltac[l] += self.centers[l]-feature[i]\n",
        "                else:\n",
        "                    deltac[l] = self.centers[l]-feature[i]\n",
        "                if l in count:\n",
        "                    count[l] += 1\n",
        "                else:\n",
        "                    count[l] = 1\n",
        "\n",
        "            for ke in deltac.keys():\n",
        "                deltac[ke] = deltac[ke]/(count[ke]+1)\n",
        "\n",
        "            return deltac\n",
        "\n",
        "    def train(self, fold):\n",
        "\n",
        "        img, label, test_data, test_label = self.get_source_data(fold)\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        label = torch.from_numpy(label + 1)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(img, label)\n",
        "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        test_data = torch.from_numpy(test_data)\n",
        "        test_label = torch.from_numpy(test_label + 1)\n",
        "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
        "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for i in range(self.c_dim):\n",
        "            self.centers[i] = torch.randn(self.dimension)\n",
        "            self.centers[i] = self.centers[i].cuda()\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        test_data = Variable(test_data.type(self.Tensor))\n",
        "        test_label = Variable(test_label.type(self.LongTensor))\n",
        "\n",
        "        bestAcc = 0\n",
        "        averAcc = 0\n",
        "        num = 0\n",
        "        Y_true = 0\n",
        "        Y_pred = 0\n",
        "\n",
        "        # Train the cnn model\n",
        "        total_step = len(self.dataloader)\n",
        "        curr_lr = self.lr\n",
        "\n",
        "        for e in range(self.n_epochs):\n",
        "            in_epoch = time.time()\n",
        "            self.model.train()\n",
        "            for i, (img, label) in enumerate(self.dataloader):\n",
        "\n",
        "                img = Variable(img.cuda().type(self.Tensor))\n",
        "                # img = self.active_function(img)\n",
        "                label = Variable(label.cuda().type(self.LongTensor))\n",
        "\n",
        "                tok, outputs = self.model(img)\n",
        "\n",
        "                # Central loss\n",
        "                cen_feature = tok\n",
        "                cen_label = label\n",
        "                nplabela = cen_label.cpu().numpy()\n",
        "\n",
        "\n",
        "                loss = self.criterion_cls(outputs, label)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            out_epoch = time.time()\n",
        "\n",
        "            if (e + 1) % 1 == 0:\n",
        "                self.model.eval()\n",
        "                Tok, Cls = self.model(test_data)\n",
        "\n",
        "                loss_test = self.criterion_cls(Cls, test_label)\n",
        "                y_pred = torch.max(Cls, 1)[1]\n",
        "                acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
        "                train_pred = torch.max(outputs, 1)[1]\n",
        "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
        "                # print('The epoch is:', e, '  The accuracy is:', acc)\n",
        "                print('Epoch:', e,\n",
        "                      '  Train loss: %.4f' % loss.detach().cpu().numpy(),\n",
        "                      '  Test loss: %.4f' % loss_test.detach().cpu().numpy(),\n",
        "                      '  Train acc: %.4f' % train_acc,\n",
        "                      '  Test acc: %.4f' % acc)\n",
        "                self.log_write.write(str(e) + \"    \" + str(acc) + \"\\n\")\n",
        "                num = num + 1\n",
        "                averAcc = averAcc + acc\n",
        "                if acc > bestAcc:\n",
        "                    bestAcc = acc\n",
        "                    Y_true = test_label\n",
        "                    Y_pred = y_pred\n",
        "\n",
        "        averAcc = averAcc / num\n",
        "        print('The average accuracy of fold%d is:' %(fold+1), averAcc)\n",
        "        print('The best accuracy of fold%d is:' %(fold+1), bestAcc)\n",
        "        self.log_write.write('The average accuracy of fold%d is: ' %(fold+1) + str(averAcc) + \"\\n\")\n",
        "        self.log_write.write('The best accuracy fold%d is: ' %(fold+1) + str(bestAcc) + \"\\n\")\n",
        "        return bestAcc, averAcc, Y_true, Y_pred\n",
        "        # writer.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    best = 0\n",
        "    aver = 0\n",
        "    result_write = open(\"/Code/CT/results/seed/5-fold/sub_result.txt\", \"w\")\n",
        "\n",
        "    for i in range(15):\n",
        "        starttime = datetime.datetime.now()\n",
        "        seed_n = np.random.randint(2021)\n",
        "\n",
        "        result_write.write('--------------------------------------------------')\n",
        "        # print('seed is ' + str(seed_n))\n",
        "        random.seed(seed_n)\n",
        "        np.random.seed(seed_n)\n",
        "        torch.manual_seed(seed_n)\n",
        "        torch.cuda.manual_seed(seed_n)\n",
        "        torch.cuda.manual_seed_all(seed_n)\n",
        "        print('Subject %d' % (i+1))\n",
        "\n",
        "        result_write.write('Subject ' + str(i + 1) + ' : ' + 'Seed is: ' + str(seed_n) + \"\\n\")\n",
        "\n",
        "        ba = 0\n",
        "        aa = 0\n",
        "        bestAcc = 0\n",
        "        averAcc = 0\n",
        "\n",
        "        for fold in range(5):\n",
        "            exgan = ExGAN(i + 1, fold)\n",
        "            ba, aa, _, _ = exgan.train(fold)\n",
        "            # print('THE BEST ACCURACY IS ' + str(ba))\n",
        "            result_write.write('Best acc of fold' + str(fold+1) + 'is: ' + str(ba) + \"\\n\")\n",
        "            result_write.write('Aver acc of fold' + str(fold+1) + 'is: ' + str(aa) + \"\\n\")\n",
        "            bestAcc += ba\n",
        "            averAcc += aa\n",
        "\n",
        "        bestAcc /= 5\n",
        "        averAcc /= 5\n",
        "        result_write.write('5-fold Best acc is: ' + str(bestAcc) + \"\\n\")\n",
        "        result_write.write('5-fold Aver acc is: ' + str(averAcc) + \"\\n\")\n",
        "        # plot_confusion_matrix(Y_true, Y_pred, i+1)\n",
        "        best = best + bestAcc\n",
        "        aver = aver + averAcc\n",
        "        endtime = datetime.datetime.now()\n",
        "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
        "\n",
        "\n",
        "    best = best / 15\n",
        "    aver = aver / 15\n",
        "\n",
        "    result_write.write('--------------------------------------------------')\n",
        "    result_write.write('All subject Best accuracy is: ' + str(best) + \"\\n\")\n",
        "    result_write.write('All subject Aver accuracy is: ' + str(aver) + \"\\n\")\n",
        "    result_write.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(time.asctime(time.localtime(time.time())))\n",
        "    main()\n",
        "    print(time.asctime(time.localtime(time.time())))"
      ]
    }
  ]
}